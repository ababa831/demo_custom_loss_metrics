{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tf.kerasのカスタムloss/metricsデモ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## サンプルデータセット用意"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-07 03:21:20.412321: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:923] could not open file to read NUMA node: /sys/bus/pci/devices/0000:09:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-01-07 03:21:20.477606: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:923] could not open file to read NUMA node: /sys/bus/pci/devices/0000:09:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-01-07 03:21:20.478037: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:923] could not open file to read NUMA node: /sys/bus/pci/devices/0000:09:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-01-07 03:21:20.479778: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-01-07 03:21:20.482417: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:923] could not open file to read NUMA node: /sys/bus/pci/devices/0000:09:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-01-07 03:21:20.482784: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:923] could not open file to read NUMA node: /sys/bus/pci/devices/0000:09:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-01-07 03:21:20.483229: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:923] could not open file to read NUMA node: /sys/bus/pci/devices/0000:09:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-01-07 03:21:21.627377: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:923] could not open file to read NUMA node: /sys/bus/pci/devices/0000:09:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-01-07 03:21:21.627752: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:923] could not open file to read NUMA node: /sys/bus/pci/devices/0000:09:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-01-07 03:21:21.627765: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1594] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2022-01-07 03:21:21.628109: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:923] could not open file to read NUMA node: /sys/bus/pci/devices/0000:09:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-01-07 03:21:21.628172: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 5980 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2060 SUPER, pci bus id: 0000:09:00.0, compute capability: 7.5\n"
     ]
    }
   ],
   "source": [
    "with tf.device(\"/CPU:0\"):\n",
    "    MAX_GROUP_ID_NUMBER = 10\n",
    "    SAMPLE_SIZE = 1000\n",
    "    target = tf.random.uniform([SAMPLE_SIZE], maxval=2, dtype=tf.int32)\n",
    "    feat_grpby = tf.random.uniform(\n",
    "        [SAMPLE_SIZE], minval=1, maxval=MAX_GROUP_ID_NUMBER + 1, dtype=tf.int32\n",
    "    )\n",
    "    feat_grpby_ohe = tf.one_hot(feat_grpby, depth=tf.reduce_max(feat_grpby))\n",
    "    feat_grouped = tf.random.normal([SAMPLE_SIZE, 2])\n",
    "\n",
    "    explains = tf.concat([feat_grouped, feat_grpby_ohe], axis=1)\n",
    "    target_w_feat_grpby = tf.stack([target, feat_grpby], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 27.42537617089975,\n",
       " 2: 3.4833571698002985,\n",
       " 3: 0.6176417763599207,\n",
       " 4: 17.3279240541167,\n",
       " 5: 10.436575357373837,\n",
       " 6: 12.52318986521875,\n",
       " 7: 23.542190564539062,\n",
       " 8: 22.196339393202983,\n",
       " 9: 20.412816578333846,\n",
       " 10: 17.838767807901256}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from random import random\n",
    "\n",
    "group_weight = {k: 30.0 * random() for k in range(1, MAX_GROUP_ID_NUMBER + 1)}\n",
    "group_weight\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE:\n",
    "\n",
    "微分可能な処理でなければならない．\n",
    "勾配情報を保持できるかは以下を参照のこと\n",
    "\n",
    "- https://www.tensorflow.org/api_docs/python/tf/raw_ops/\n",
    "- https://stackoverflow.com/a/44575034\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "\n",
    "from tensorflow.keras.losses import Loss, binary_crossentropy as bce\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "class GroupWeightedBinaryCrossentropy(Loss):\n",
    "    def __init__(self, group_weight: dict, name=\"GroupWeightedBCE\"):\n",
    "        super().__init__(name=name)\n",
    "        self.name = name\n",
    "\n",
    "        if not isinstance(group_weight, dict):\n",
    "            errmsg = \"For the feature column to be grouped, \"\n",
    "            errmsg += \"give the weights of each ID as arguments in dict format.\"\n",
    "            raise TypeError(errmsg)\n",
    "        self.group_weight = group_weight\n",
    "\n",
    "        self.avg_loss_grpby = {\n",
    "            k: tf.convert_to_tensor(0, dtype=tf.float32) for k in group_weight.keys()\n",
    "        }\n",
    "        self.global_avg_loss = tf.convert_to_tensor(0, dtype=tf.float32)\n",
    "        self.batch_cnt = tf.convert_to_tensor(0, dtype=tf.float32) \n",
    "\n",
    "    def _update_loss_for_each_group(self, y_true_w_id, y_pred):\n",
    "        y_true = y_true_w_id[:, 0]\n",
    "        feat_id = y_true_w_id[:, 1]\n",
    "\n",
    "        y_pred = tf.convert_to_tensor(y_pred)\n",
    "        y_pred = tf.cast(y_pred, tf.float32)\n",
    "        y_pred = tf.reshape(y_pred, shape=y_true.shape)\n",
    "        y_true = tf.cast(y_true, y_pred.dtype)\n",
    "\n",
    "        # Group内，IDごとの平均lossを計算\n",
    "        for k in self.group_weight.keys():\n",
    "            selected_indices = tf.where(tf.equal(feat_id, k))\n",
    "            # NaN対策（IDが存在しない場合）\n",
    "            if selected_indices.shape[0] < 1:\n",
    "                continue\n",
    "            y_pred_grpby = tf.gather_nd(y_pred, selected_indices)\n",
    "            y_true_grpby = tf.gather_nd(y_true, selected_indices)\n",
    "            y_pred_grpby = tf.reshape(y_pred_grpby, shape=[1, -1])\n",
    "            y_true_grpby = tf.reshape(y_true_grpby, shape=[1, -1])\n",
    "            loss_for_group = bce(y_true_grpby, y_pred_grpby)[0]\n",
    "\n",
    "            multiplied = tf.multiply(self.avg_loss_grpby[k], self.batch_cnt)\n",
    "            numerator = tf.add(multiplied, loss_for_group)\n",
    "            denominator = tf.add(self.batch_cnt, 1)\n",
    "            self.avg_loss_grpby[k] = tf.divide(numerator, denominator)\n",
    "\n",
    "        self.batch_cnt = tf.add(self.batch_cnt, 1)\n",
    "\n",
    "    def _update_global_loss(self, avg_loss_grpby, group_weight: dict):\n",
    "        sum_loss_weighted = tf.convert_to_tensor(0, dtype=tf.float32)\n",
    "        denominator = tf.convert_to_tensor(0, dtype=tf.float32)\n",
    "        for k, avg_loss in avg_loss_grpby.items():\n",
    "            weight = group_weight[k]\n",
    "            multiplied = tf.multiply(avg_loss, weight)\n",
    "            sum_loss_weighted = tf.add(sum_loss_weighted, multiplied)\n",
    "            denominator = tf.add(denominator, weight)\n",
    "\n",
    "        self.global_avg_loss = tf.divide(sum_loss_weighted, denominator)\n",
    "\n",
    "    def call(self, y_true_w_id, y_pred):\n",
    "        # model.compile, fit時の内部挙動でエラーが吐きそうなため\n",
    "        # y_trueはdictではなくarray likeに与える\n",
    "        self._update_loss_for_each_group(y_true_w_id, y_pred)\n",
    "        self._update_global_loss(self.avg_loss_grpby, self.group_weight)\n",
    "\n",
    "        return self.global_avg_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=5.503593>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 計算値確認\n",
    "tmp = GroupWeightedBinaryCrossentropy(group_weight=group_weight)\n",
    "y_pred = tf.random.normal(shape=[target_w_feat_grpby.shape[0]])\n",
    "global_weighted_loss = tmp.call(target_w_feat_grpby, y_pred)\n",
    "global_weighted_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 重み付け確認\n",
    "wo_weight = np.mean([v.numpy() for v in tmp.avg_loss_grpby.values()])\n",
    "\n",
    "assert wo_weight != global_weighted_loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NG1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfrom typing import Optional\\n\\nfrom tensorflow.keras.losses import Loss\\nimport pandas as pd\\n\\n\\nclass GroupWeightedDeviation(Loss):\\n    def __init__(self, group_weight: dict, name=\"Deviation\"):\\n        super().__init__(name=name)\\n        self.name = name\\n        if isinstance(group_weight, dict):\\n            self.group_weight = pd.Series(group_weight, name=\"GroupWeight\")\\n            self.group_weight.sort_index(ascending=True, inplace=True)\\n        else:\\n            errmsg = \"For the feature column to be grouped, \"\\n            errmsg += \"give the weights of each ID as arguments in dict format.\"\\n            raise TypeError(errmsg)\\n        self.global_avg_loss = 0.0\\n        self.batch_cnt = 0\\n        self.avg_loss_grpby = pd.Series(0.0, index=group_weight.keys(), name=name)\\n        self.avg_loss_grpby.sort_index(ascending=True, inplace=True)\\n\\n    def call(self, y_true_w_id, y_pred, var_deal_zerodiv=0.0001):\\n        # model.compile, fit時の内部挙動でエラーが吐きそうなため\\n        # y_trueはdictではなくarray likeに与える\\n        y_true = y_true_w_id[:, 0]\\n        feat_id = y_true_w_id[:, 1]\\n\\n        y_pred = tf.convert_to_tensor(y_pred)\\n        y_pred = tf.cast(y_pred, tf.float32)\\n        y_pred = tf.reshape(y_pred, shape=y_true.shape)\\n        y_true = tf.cast(y_true, y_pred.dtype)\\n\\n        deviation = tf.math.abs(y_true / (y_pred + var_deal_zerodiv) - 1)\\n        loss = pd.Series(deviation.numpy(), index=feat_id, name=self.name)\\n        loss = loss.groupby(by=loss.index).mean()\\n\\n        # 単純な+で演算した場合，avg_loss_grpbyのIDがlossに含まれていない場合\\n        # 該当IDの演算和がNaNになるため fill_valを指定したaddを利用\\n        denom = (self.avg_loss_grpby * self.batch_cnt).add(loss, fill_value=0.0)\\n        self.avg_loss_grpby = denom / (self.batch_cnt + 1)\\n\\n        self.global_avg_loss = (\\n            self.avg_loss_grpby * self.group_weight\\n        ).sum() / self.group_weight.sum()\\n\\n        self.batch_cnt += 1\\n\\n        return tf.convert_to_tensor(self.global_avg_loss, dtype=tf.float32)\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "from typing import Optional\n",
    "\n",
    "from tensorflow.keras.losses import Loss\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "class GroupWeightedDeviation(Loss):\n",
    "    def __init__(self, group_weight: dict, name=\"Deviation\"):\n",
    "        super().__init__(name=name)\n",
    "        self.name = name\n",
    "        if isinstance(group_weight, dict):\n",
    "            self.group_weight = pd.Series(group_weight, name=\"GroupWeight\")\n",
    "            self.group_weight.sort_index(ascending=True, inplace=True)\n",
    "        else:\n",
    "            errmsg = \"For the feature column to be grouped, \"\n",
    "            errmsg += \"give the weights of each ID as arguments in dict format.\"\n",
    "            raise TypeError(errmsg)\n",
    "        self.global_avg_loss = 0.0\n",
    "        self.batch_cnt = 0\n",
    "        self.avg_loss_grpby = pd.Series(0.0, index=group_weight.keys(), name=name)\n",
    "        self.avg_loss_grpby.sort_index(ascending=True, inplace=True)\n",
    "\n",
    "    def call(self, y_true_w_id, y_pred, var_deal_zerodiv=0.0001):\n",
    "        # model.compile, fit時の内部挙動でエラーが吐きそうなため\n",
    "        # y_trueはdictではなくarray likeに与える\n",
    "        y_true = y_true_w_id[:, 0]\n",
    "        feat_id = y_true_w_id[:, 1]\n",
    "\n",
    "        y_pred = tf.convert_to_tensor(y_pred)\n",
    "        y_pred = tf.cast(y_pred, tf.float32)\n",
    "        y_pred = tf.reshape(y_pred, shape=y_true.shape)\n",
    "        y_true = tf.cast(y_true, y_pred.dtype)\n",
    "\n",
    "        deviation = tf.math.abs(y_true / (y_pred + var_deal_zerodiv) - 1)\n",
    "        loss = pd.Series(deviation.numpy(), index=feat_id, name=self.name)\n",
    "        loss = loss.groupby(by=loss.index).mean()\n",
    "\n",
    "        # 単純な+で演算した場合，avg_loss_grpbyのIDがlossに含まれていない場合\n",
    "        # 該当IDの演算和がNaNになるため fill_valを指定したaddを利用\n",
    "        denom = (self.avg_loss_grpby * self.batch_cnt).add(loss, fill_value=0.0)\n",
    "        self.avg_loss_grpby = denom / (self.batch_cnt + 1)\n",
    "\n",
    "        self.global_avg_loss = (\n",
    "            self.avg_loss_grpby * self.group_weight\n",
    "        ).sum() / self.group_weight.sum()\n",
    "\n",
    "        self.batch_cnt += 1\n",
    "\n",
    "        return tf.convert_to_tensor(self.global_avg_loss, dtype=tf.float32)\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NG2\n",
    "\n",
    "逆伝搬の勾配情報が保持できなければcustom lossを確保できないので，以下は対応していない\n",
    "\n",
    "https://www.tensorflow.org/api_docs/python/tf/raw_ops/\n",
    "で確認できる\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfrom typing import Optional\\n\\nfrom tensorflow.keras.losses import Loss, binary_crossentropy as bce\\nimport pandas as pd\\n\\n\\nclass GroupWeightedBinaryCrossentropy(Loss):\\n    def __init__(self, group_weight: dict, name=\"Deviation\"):\\n        super().__init__(name=name)\\n        self.name = name\\n        if isinstance(group_weight, dict):\\n            self.group_weight = pd.Series(\\n                group_weight, name=\"GroupWeight\", dtype=np.float32\\n            )\\n            self.group_weight.sort_index(ascending=True, inplace=True)\\n        else:\\n            errmsg = \"For the feature column to be grouped, \"\\n            errmsg += \"give the weights of each ID as arguments in dict format.\"\\n            raise TypeError(errmsg)\\n        self.global_avg_loss = 0.0\\n        self.batch_cnt = 0\\n        self.avg_loss_grpby = pd.Series(0.0, index=group_weight.keys(), name=name)\\n        self.avg_loss_grpby.sort_index(ascending=True, inplace=True)\\n\\n    def call(self, y_true_w_id, y_pred):\\n        # model.compile, fit時の内部挙動でエラーが吐きそうなため\\n        # y_trueはdictではなくarray likeに与える\\n        y_true = y_true_w_id[:, 0]\\n        feat_id = y_true_w_id[:, 1]\\n\\n        y_pred = tf.convert_to_tensor(y_pred)\\n        y_pred = tf.cast(y_pred, tf.float32)\\n        y_pred = tf.reshape(y_pred, shape=y_true.shape)\\n        y_true = tf.cast(y_true, y_pred.dtype)\\n\\n        loss = pd.DataFrame({\"y_true\": y_true, \"y_pred\": y_pred}, index=feat_id)\\n        loss = loss.groupby(by=loss.index).apply(lambda d: bce(d.y_true, d.y_pred))\\n\\n        # 単純な+で演算した場合，avg_loss_grpbyのIDがlossに含まれていない場合\\n        # 該当IDの演算和がNaNになるため fill_valを指定したaddを利用\\n        denom = (self.avg_loss_grpby * self.batch_cnt).add(loss, fill_value=tf.constant(0.0, dtype=tf.float32, shape=[0]))\\n        #display(denom)\\n        self.avg_loss_grpby = denom / (self.batch_cnt + 1)\\n\\n        denom = tf.reduce_sum(self.avg_loss_grpby * self.group_weight)\\n        self.global_avg_loss = tf.divide(denom, self.group_weight.sum())\\n\\n        self.batch_cnt += 1\\n\\n        return self.global_avg_loss\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "from typing import Optional\n",
    "\n",
    "from tensorflow.keras.losses import Loss, binary_crossentropy as bce\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "class GroupWeightedBinaryCrossentropy(Loss):\n",
    "    def __init__(self, group_weight: dict, name=\"Deviation\"):\n",
    "        super().__init__(name=name)\n",
    "        self.name = name\n",
    "        if isinstance(group_weight, dict):\n",
    "            self.group_weight = pd.Series(\n",
    "                group_weight, name=\"GroupWeight\", dtype=np.float32\n",
    "            )\n",
    "            self.group_weight.sort_index(ascending=True, inplace=True)\n",
    "        else:\n",
    "            errmsg = \"For the feature column to be grouped, \"\n",
    "            errmsg += \"give the weights of each ID as arguments in dict format.\"\n",
    "            raise TypeError(errmsg)\n",
    "        self.global_avg_loss = 0.0\n",
    "        self.batch_cnt = 0\n",
    "        self.avg_loss_grpby = pd.Series(0.0, index=group_weight.keys(), name=name)\n",
    "        self.avg_loss_grpby.sort_index(ascending=True, inplace=True)\n",
    "\n",
    "    def call(self, y_true_w_id, y_pred):\n",
    "        # model.compile, fit時の内部挙動でエラーが吐きそうなため\n",
    "        # y_trueはdictではなくarray likeに与える\n",
    "        y_true = y_true_w_id[:, 0]\n",
    "        feat_id = y_true_w_id[:, 1]\n",
    "\n",
    "        y_pred = tf.convert_to_tensor(y_pred)\n",
    "        y_pred = tf.cast(y_pred, tf.float32)\n",
    "        y_pred = tf.reshape(y_pred, shape=y_true.shape)\n",
    "        y_true = tf.cast(y_true, y_pred.dtype)\n",
    "\n",
    "        loss = pd.DataFrame({\"y_true\": y_true, \"y_pred\": y_pred}, index=feat_id)\n",
    "        loss = loss.groupby(by=loss.index).apply(lambda d: bce(d.y_true, d.y_pred))\n",
    "\n",
    "        # 単純な+で演算した場合，avg_loss_grpbyのIDがlossに含まれていない場合\n",
    "        # 該当IDの演算和がNaNになるため fill_valを指定したaddを利用\n",
    "        denom = (self.avg_loss_grpby * self.batch_cnt).add(loss, fill_value=tf.constant(0.0, dtype=tf.float32, shape=[0]))\n",
    "        #display(denom)\n",
    "        self.avg_loss_grpby = denom / (self.batch_cnt + 1)\n",
    "\n",
    "        denom = tf.reduce_sum(self.avg_loss_grpby * self.group_weight)\n",
    "        self.global_avg_loss = tf.divide(denom, self.group_weight.sum())\n",
    "\n",
    "        self.batch_cnt += 1\n",
    "\n",
    "        return self.global_avg_loss\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 適当なモデルを用意"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.dense1 = tf.keras.layers.Dense(4, activation=tf.nn.relu)\n",
    "        self.dense2 = tf.keras.layers.Dense(1, activation=tf.nn.softmax)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.dense1(inputs)\n",
    "        return self.dense2(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MyModel()\n",
    "# TODO: 適当にTrainする\n",
    "model.compile(\n",
    "    loss=GroupWeightedBinaryCrossentropy(group_weight=group_weight),\n",
    "    optimizer=\"adam\",\n",
    "    run_eagerly=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 学習"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train-test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "train_indices, valid_indices = train_test_split(np.arange(SAMPLE_SIZE), train_size=0.9)\n",
    "\n",
    "with tf.device(\"/CPU:0\"):\n",
    "    X_train = tf.gather(explains, train_indices, axis=0)\n",
    "    X_valid = tf.gather(explains, valid_indices, axis=0)\n",
    "    y_train_w_feat_grpby = tf.gather(target_w_feat_grpby, train_indices, axis=0)\n",
    "    y_valid_w_feat_grpby = tf.gather(target_w_feat_grpby, valid_indices, axis=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1/29 [>.............................] - ETA: 3s - loss: 9.1056"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-07 03:21:23.555740: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29/29 [==============================] - 1s 29ms/step - loss: 7.9877 - val_loss: 7.5598\n"
     ]
    }
   ],
   "source": [
    "with tf.device(\"/CPU:0\"):\n",
    "    model.fit(\n",
    "        X_train,\n",
    "        y_train_w_feat_grpby,\n",
    "        validation_data=(X_valid, y_valid_w_feat_grpby),\n",
    "        epochs=1,\n",
    "        batch_size=32,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7c1609ba26b998026eb8d2a3c26cb7c12e1a2d62482ef79eb5f0c72a03838677"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit ('.venv': poetry)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
