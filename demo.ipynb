{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tf.kerasのカスタムloss/metricsデモ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## サンプルデータセット用意"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device(\"/CPU:0\"):\n",
    "    MAX_GROUP_ID_NUMBER = 10\n",
    "    SAMPLE_SIZE = 1000\n",
    "    target = tf.random.uniform([SAMPLE_SIZE], maxval=2, dtype=tf.int32)\n",
    "    feat_grpby = tf.random.uniform(\n",
    "        [SAMPLE_SIZE], minval=1, maxval=MAX_GROUP_ID_NUMBER + 1, dtype=tf.int32\n",
    "    )\n",
    "    feat_grpby_ohe = tf.one_hot(feat_grpby, depth=tf.reduce_max(feat_grpby))\n",
    "    feat_grouped = tf.random.normal([SAMPLE_SIZE, 2])\n",
    "\n",
    "    explains = tf.concat([feat_grouped, feat_grpby_ohe], axis=1)\n",
    "    target_w_feat_grpby = tf.stack([target, feat_grpby], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 15.422958906298708,\n",
       " 2: 28.35185428831561,\n",
       " 3: 6.526315065849767,\n",
       " 4: 9.755581898876136,\n",
       " 5: 17.092259540150007,\n",
       " 6: 1.2714226403896756,\n",
       " 7: 24.640910300692433,\n",
       " 8: 20.271732513558447,\n",
       " 9: 2.705301692460056,\n",
       " 10: 7.096694648754539}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from random import random\n",
    "\n",
    "group_weight = {k: 30.0 * random() for k in range(1, MAX_GROUP_ID_NUMBER + 1)}\n",
    "group_weight\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE:\n",
    "\n",
    "微分可能な処理でなければならない．\n",
    "勾配情報を保持できるかは以下を参照のこと\n",
    "\n",
    "- https://www.tensorflow.org/api_docs/python/tf/raw_ops/\n",
    "- https://stackoverflow.com/a/44575034\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "\n",
    "from tensorflow.keras.losses import Loss, binary_crossentropy as bce\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "class GroupWeightedBinaryCrossentropy(Loss):\n",
    "    def __init__(self, group_weight: dict, name=\"GroupWeightedBCE\"):\n",
    "        super().__init__(name=name)\n",
    "        self.name = name\n",
    "\n",
    "        if not isinstance(group_weight, dict):\n",
    "            errmsg = \"For the feature column to be grouped, \"\n",
    "            errmsg += \"give the weights of each ID as arguments in dict format.\"\n",
    "            raise TypeError(errmsg)\n",
    "        self.group_weight = group_weight\n",
    "\n",
    "        self.avg_loss_grpby = {\n",
    "            k: tf.convert_to_tensor(0, dtype=tf.float32) for k in group_weight.keys()\n",
    "        }\n",
    "        self.global_avg_loss = tf.convert_to_tensor(0, dtype=tf.float32)\n",
    "        self.batch_cnt = tf.convert_to_tensor(0, dtype=tf.float32) \n",
    "\n",
    "    def _update_loss_for_each_group(self, y_true_w_id, y_pred):\n",
    "        y_true = y_true_w_id[:, 0]\n",
    "        feat_id = y_true_w_id[:, 1]\n",
    "\n",
    "        y_pred = tf.convert_to_tensor(y_pred)\n",
    "        y_pred = tf.cast(y_pred, tf.float32)\n",
    "        y_pred = tf.reshape(y_pred, shape=y_true.shape)\n",
    "        y_true = tf.cast(y_true, y_pred.dtype)\n",
    "\n",
    "        # Group内，IDごとの平均lossを計算\n",
    "        for k in self.group_weight.keys():\n",
    "            selected_indices = tf.where(tf.equal(feat_id, k))\n",
    "            # NaN対策（IDが存在しない場合）\n",
    "            if selected_indices.shape[0] < 1:\n",
    "                continue\n",
    "            y_pred_grpby = tf.gather_nd(y_pred, selected_indices)\n",
    "            y_true_grpby = tf.gather_nd(y_true, selected_indices)\n",
    "            y_pred_grpby = tf.reshape(y_pred_grpby, shape=[1, -1])\n",
    "            y_true_grpby = tf.reshape(y_true_grpby, shape=[1, -1])\n",
    "            loss_for_group = bce(y_true_grpby, y_pred_grpby)[0]\n",
    "\n",
    "            multiplied = tf.multiply(self.avg_loss_grpby[k], self.batch_cnt)\n",
    "            numerator = tf.add(multiplied, loss_for_group)\n",
    "            denominator = tf.add(self.batch_cnt, 1)\n",
    "            self.avg_loss_grpby[k] = tf.divide(numerator, denominator)\n",
    "\n",
    "        self.batch_cnt = tf.add(self.batch_cnt, 1)\n",
    "\n",
    "    def _update_global_loss(self, avg_loss_grpby, group_weight: dict):\n",
    "        sum_loss_weighted = tf.convert_to_tensor(0, dtype=tf.float32)\n",
    "        denominator = tf.convert_to_tensor(0, dtype=tf.float32)\n",
    "        for k, avg_loss in avg_loss_grpby.items():\n",
    "            weight = group_weight[k]\n",
    "            multiplied = tf.multiply(avg_loss, weight)\n",
    "            sum_loss_weighted = tf.add(sum_loss_weighted, multiplied)\n",
    "            denominator = tf.add(denominator, weight)\n",
    "\n",
    "        self.global_avg_loss = tf.divide(sum_loss_weighted, denominator)\n",
    "\n",
    "    def call(self, y_true_w_id, y_pred):\n",
    "        # model.compile, fit時の内部挙動でエラーが吐きそうなため\n",
    "        # y_trueはdictではなくarray likeに与える\n",
    "        self._update_loss_for_each_group(y_true_w_id, y_pred)\n",
    "        self._update_global_loss(self.avg_loss_grpby, self.group_weight)\n",
    "\n",
    "        return self.global_avg_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=5.236294>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 計算値確認\n",
    "tmp = GroupWeightedBinaryCrossentropy(group_weight=group_weight)\n",
    "y_pred = tf.random.normal(shape=[target_w_feat_grpby.shape[0]])\n",
    "global_weighted_loss = tmp.call(target_w_feat_grpby, y_pred)\n",
    "global_weighted_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 重み付け確認\n",
    "wo_weight = np.mean([v.numpy() for v in tmp.avg_loss_grpby.values()])\n",
    "\n",
    "assert wo_weight != global_weighted_loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 適当なモデルを用意"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.dense1 = tf.keras.layers.Dense(4, activation=tf.nn.relu)\n",
    "        self.dense2 = tf.keras.layers.Dense(1, activation=tf.nn.softmax)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.dense1(inputs)\n",
    "        return self.dense2(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MyModel()\n",
    "# TODO: 適当にTrainする\n",
    "model.compile(\n",
    "    loss=GroupWeightedBinaryCrossentropy(group_weight=group_weight),\n",
    "    optimizer=\"adam\",\n",
    "    run_eagerly=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 学習"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train-test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "train_indices, valid_indices = train_test_split(np.arange(SAMPLE_SIZE), train_size=0.9)\n",
    "\n",
    "with tf.device(\"/CPU:0\"):\n",
    "    X_train = tf.gather(explains, train_indices, axis=0)\n",
    "    X_valid = tf.gather(explains, valid_indices, axis=0)\n",
    "    y_train_w_feat_grpby = tf.gather(target_w_feat_grpby, train_indices, axis=0)\n",
    "    y_valid_w_feat_grpby = tf.gather(target_w_feat_grpby, valid_indices, axis=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29/29 [==============================] - 1s 30ms/step - loss: 7.4394 - val_loss: 7.7780\n"
     ]
    }
   ],
   "source": [
    "with tf.device(\"/CPU:0\"):\n",
    "    model.fit(\n",
    "        X_train,\n",
    "        y_train_w_feat_grpby,\n",
    "        validation_data=(X_valid, y_valid_w_feat_grpby),\n",
    "        epochs=1,\n",
    "        batch_size=32,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7c1609ba26b998026eb8d2a3c26cb7c12e1a2d62482ef79eb5f0c72a03838677"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit ('.venv': poetry)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
